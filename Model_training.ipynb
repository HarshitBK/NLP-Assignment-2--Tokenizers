{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T64Q8dIsV0j2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertConfig, BertForMaskedLM, AutoTokenizer, AdamW, get_scheduler\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Custom Dataset to load tokenized data from a text file\n",
        "class TokenizedTextDataset(Dataset):\n",
        "    def _init_(self, file_path, tokenizer, max_length=512):\n",
        "        self.input_ids = []\n",
        "        self.attention_masks = []\n",
        "\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                tokens = list(map(int, line.strip().split()))\n",
        "                attention_mask = [1] * len(tokens)\n",
        "\n",
        "                # Ensure all tokens are within the model's vocabulary size\n",
        "                tokens = [min(token, tokenizer.vocab_size - 1) for token in tokens]\n",
        "\n",
        "                # Pad or truncate the sequences to max_length\n",
        "                if len(tokens) < max_length:\n",
        "                    padding_length = max_length - len(tokens)\n",
        "                    tokens += [tokenizer.pad_token_id] * padding_length\n",
        "                    attention_mask += [0] * padding_length\n",
        "                else:\n",
        "                    tokens = tokens[:max_length]\n",
        "                    attention_mask = attention_mask[:max_length]\n",
        "\n",
        "                self.input_ids.append(tokens)\n",
        "                self.attention_masks.append(attention_mask)\n",
        "\n",
        "    def _len_(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def _getitem_(self, idx):\n",
        "        return {\n",
        "            \"input_ids\": torch.tensor(self.input_ids[idx]),\n",
        "            \"attention_mask\": torch.tensor(self.attention_masks[idx]),\n",
        "        }\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Load tokenized dataset from the .txt file\n",
        "file_path = \"/kaggle/input/tokenized-dataset/Tokenized_Data.txt\"\n",
        "dataset = TokenizedTextDataset(file_path, tokenizer)\n",
        "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "# Define BERT configuration\n",
        "bert_config = BertConfig(\n",
        "    num_hidden_layers=8,\n",
        "    hidden_size=200,\n",
        "    intermediate_size=1024,\n",
        "    num_attention_heads=8,\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_position_embeddings=512\n",
        ")\n",
        "\n",
        "# Initialize the model for Masked Language Modeling\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = BertForMaskedLM(bert_config)\n",
        "\n",
        "# Convert model to GPU and set it to half-precision (float16)\n",
        "model = model.to(device)\n",
        "model = model.half()\n",
        "\n",
        "# Wrap model in a low-rank adaptation (LoRA) configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=8,  # Rank of the low-rank matrices\n",
        "    lora_alpha=16,  # Scaling parameter\n",
        "    target_modules=[\"query\", \"value\"],  # Apply LoRA on attention query/value projections\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\"\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "print(f\"Total Parameters with LoRA: {sum(p.numel() for p in model.parameters())}\")\n",
        "print(f\"Trainable Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")\n",
        "\n",
        "# Optimizer using AdamW optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(dataloader) * 3)\n",
        "\n",
        "# Mixed-precision training setup (using PyTorch AMP)\n",
        "scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "# Logging\n",
        "with open(\"perplexity_log.txt\", \"w\") as f:\n",
        "    f.write(\"Epoch, Step, Perplexity\\n\")\n",
        "\n",
        "# Training Loop\n",
        "model.train()\n",
        "for epoch in range(3):\n",
        "    progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/3\")\n",
        "\n",
        "    total_loss = 0\n",
        "    total_steps = 0\n",
        "\n",
        "    # Calculate the number of steps for 0.1 epoch\n",
        "    steps_per_epoch = len(dataloader)\n",
        "    steps_per_10_percent_epoch = steps_per_epoch // 10\n",
        "\n",
        "    for batch in progress_bar:\n",
        "        input_ids, attention_mask = batch[\"input_ids\"].to(device), batch[\"attention_mask\"].to(device)\n",
        "\n",
        "        # Forward pass with mixed precision\n",
        "        with torch.cuda.amp.autocast():\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "        scheduler.step()\n",
        "\n",
        "        # Track loss\n",
        "        total_loss += loss.item()\n",
        "        total_steps += 1\n",
        "\n",
        "        progress_bar.set_postfix({\"loss\": loss.item()})\n",
        "\n",
        "        # Calculate and print perplexity every 0.1 epoch\n",
        "        if total_steps % steps_per_10_percent_epoch == 0:\n",
        "            avg_loss = total_loss / total_steps\n",
        "            perplexity = math.exp(avg_loss)\n",
        "            print(f\"Epoch {epoch + 1}, Step {total_steps}/{steps_per_epoch}, Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "            # Log perplexity\n",
        "            with open(\"perplexity_log.txt\", \"a\") as f:\n",
        "                f.write(f\"{epoch + 1}, {total_steps}, {perplexity:.2f}\\n\")\n",
        "\n",
        "    avg_loss = total_loss / total_steps\n",
        "    perplexity = math.exp(avg_loss)\n",
        "    print(f\"Epoch {epoch + 1} Perplexity: {perplexity:.2f}\")\n",
        "\n",
        "# Save model\n",
        "model.save_pretrained(\"quantized_lora_bert\")\n",
        "print(\"Training complete! Model saved at 'quantized_lora_bert'.\")"
      ]
    }
  ]
}